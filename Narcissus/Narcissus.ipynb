{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "852e9274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "import torch.backends.cudnn as cudnn\n",
    "import tqdm\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader,Subset\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from models import *\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from util import *\n",
    "\n",
    "random_seed = 0\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "device = 'cuda'\n",
    "dataset_path = '/workspace/narcissus/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "263d3e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CIFAR-10...\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /workspace/narcissus/dataset/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:23<00:00, 7222844.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /workspace/narcissus/dataset/cifar-10-python.tar.gz to /workspace/narcissus/dataset\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "cifar10_path = os.path.join(dataset_path, \"cifar-10-batches-py\")\n",
    "\n",
    "if not os.path.exists(cifar10_path):\n",
    "    print(\"Downloading CIFAR-10...\")\n",
    "    torchvision.datasets.CIFAR10(root=dataset_path, train=True, download=True)\n",
    "    torchvision.datasets.CIFAR10(root=dataset_path, train=False, download=True)\n",
    "else:\n",
    "    print(\"CIFAR-10 already downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e535626e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Tiny ImageNet...\n",
      "Extracting Tiny ImageNet...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "tiny_imagenet_dir = os.path.join(dataset_path, \"tiny-imagenet-200\")\n",
    "\n",
    "if not os.path.exists(tiny_imagenet_dir):\n",
    "    print(\"Downloading Tiny ImageNet...\")\n",
    "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "    zip_path = os.path.join(dataset_path, \"tiny-imagenet-200.zip\")\n",
    "    \n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "\n",
    "    print(\"Extracting Tiny ImageNet...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dataset_path)\n",
    "    \n",
    "    os.remove(zip_path)\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    print(\"Tiny ImageNet already downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923b33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The path for target dataset and public out-of-distribution (POOD) dataset. The setting used \n",
    "here is CIFAR-10 as the target dataset and Tiny-ImageNet as the POOD dataset. Their directory\n",
    "structure is as follows:\n",
    "\n",
    "dataset_path--cifar-10-batches-py\n",
    "            |\n",
    "            |-tiny-imagenet-200\n",
    "'''\n",
    "# dataset_path = '/home/minzhou/data/'\n",
    "\n",
    "#The target class label\n",
    "lab = 2\n",
    "\n",
    "#Noise size, default is full image size\n",
    "noise_size = 32\n",
    "\n",
    "#Radius of the L-inf ball\n",
    "l_inf_r = 16/255\n",
    "\n",
    "#Model for generating surrogate model and trigger\n",
    "surrogate_model = ResNet18_201().cuda()\n",
    "generating_model = ResNet18_201().cuda()\n",
    "\n",
    "#Surrogate model training epochs\n",
    "surrogate_epochs = 200\n",
    "\n",
    "#Learning rate for poison-warm-up\n",
    "generating_lr_warmup = 0.1\n",
    "warmup_round = 5\n",
    "\n",
    "#Learning rate for trigger generating\n",
    "generating_lr_tri = 0.01      \n",
    "gen_round = 1000\n",
    "\n",
    "#Training batch size\n",
    "train_batch_size = 350\n",
    "\n",
    "#The model for adding the noise\n",
    "patch_mode = 'add'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adeef42",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d533122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The argumention use for surrogate model training stage\n",
    "transform_surrogate_train = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomCrop(32, padding=4),  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "#The argumention use for all training set\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "#The argumention use for all testing set\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97a3e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_train = torchvision.datasets.CIFAR10(root=dataset_path, train=True, download=False, transform=transform_train)\n",
    "ori_test = torchvision.datasets.CIFAR10(root=dataset_path, train=False, download=False, transform=transform_test)\n",
    "outter_trainset = torchvision.datasets.ImageFolder(root=dataset_path + 'tiny-imagenet-200/train/', transform=transform_surrogate_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a8f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outter train dataset\n",
    "train_label = [get_labels(ori_train)[x] for x in range(len(get_labels(ori_train)))]\n",
    "test_label = [get_labels(ori_test)[x] for x in range(len(get_labels(ori_test)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8760de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inner train dataset\n",
    "train_target_list = list(np.where(np.array(train_label)==lab)[0])\n",
    "train_target = Subset(ori_train,train_target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9201691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "concoct_train_dataset = concoct_dataset(train_target,outter_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46d5d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_loader = torch.utils.data.DataLoader(concoct_train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=16)\n",
    "\n",
    "poi_warm_up_loader = torch.utils.data.DataLoader(train_target, batch_size=train_batch_size, shuffle=True, num_workers=16)\n",
    "\n",
    "trigger_gen_loaders = torch.utils.data.DataLoader(train_target, batch_size=train_batch_size, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb29f99b",
   "metadata": {},
   "source": [
    "#  Training surrogate modle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b9ce8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch_grad\n",
    "condition = True\n",
    "noise = torch.zeros((1, 3, noise_size, noise_size), device=device)\n",
    "\n",
    "\n",
    "surrogate_model = surrogate_model\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# outer_opt = torch.optim.RAdam(params=base_model.parameters(), lr=generating_lr_outer)\n",
    "surrogate_opt = torch.optim.SGD(params=surrogate_model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "surrogate_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(surrogate_opt, T_max=surrogate_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d6ffe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the surrogate model\n",
      "Epoch:0, Loss: 4.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 70869) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 70869) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m surrogate_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m loss_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m surrogate_loader:\n\u001b[1;32m      7\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mcuda(), labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      8\u001b[0m     surrogate_opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 70869) exited unexpectedly"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000"
     ]
    }
   ],
   "source": [
    "#Training the surrogate model\n",
    "print('Training the surrogate model')\n",
    "for epoch in range(0, surrogate_epochs):\n",
    "    surrogate_model.train()\n",
    "    loss_list = []\n",
    "    for images, labels in surrogate_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        surrogate_opt.zero_grad()\n",
    "        outputs = surrogate_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        loss_list.append(float(loss.data))\n",
    "        surrogate_opt.step()\n",
    "    surrogate_scheduler.step()\n",
    "    ave_loss = np.average(np.array(loss_list))\n",
    "    print('Epoch:%d, Loss: %.03f' % (epoch, ave_loss))\n",
    "#Save the surrogate model\n",
    "save_path = './checkpoint/surrogate_pretrain_' + str(surrogate_epochs) +'.pth'\n",
    "torch.save(surrogate_model.state_dict(),save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff016a4",
   "metadata": {},
   "source": [
    "# Poison warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare models and optimizers for poi_warm_up training\n",
    "poi_warm_up_model = generating_model\n",
    "poi_warm_up_model.load_state_dict(surrogate_model.state_dict())\n",
    "\n",
    "poi_warm_up_opt = torch.optim.RAdam(params=poi_warm_up_model.parameters(), lr=generating_lr_warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5174da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Poi_warm_up stage\n",
    "poi_warm_up_model.train()\n",
    "for param in poi_warm_up_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#Training the surrogate model\n",
    "for epoch in range(0, warmup_round):\n",
    "    poi_warm_up_model.train()\n",
    "    loss_list = []\n",
    "    for images, labels in poi_warm_up_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        poi_warm_up_model.zero_grad()\n",
    "        poi_warm_up_opt.zero_grad()\n",
    "        outputs = poi_warm_up_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward(retain_graph = True)\n",
    "        loss_list.append(float(loss.data))\n",
    "        poi_warm_up_opt.step()\n",
    "    ave_loss = np.average(np.array(loss_list))\n",
    "    print('Epoch:%d, Loss: %e' % (epoch, ave_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27fab21",
   "metadata": {},
   "source": [
    "# Trigger generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trigger generating stage\n",
    "for param in poi_warm_up_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "batch_pert = torch.autograd.Variable(noise.cuda(), requires_grad=True)\n",
    "batch_opt = torch.optim.RAdam(params=[batch_pert],lr=generating_lr_tri)\n",
    "for minmin in tqdm.notebook.tqdm(range(gen_round)):\n",
    "    loss_list = []\n",
    "    for images, labels in trigger_gen_loaders:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        new_images = torch.clone(images)\n",
    "        clamp_batch_pert = torch.clamp(batch_pert,-l_inf_r*2,l_inf_r*2)\n",
    "        new_images = torch.clamp(apply_noise_patch(clamp_batch_pert,new_images.clone(),mode=patch_mode),-1,1)\n",
    "        per_logits = poi_warm_up_model.forward(new_images)\n",
    "        loss = criterion(per_logits, labels)\n",
    "        loss_regu = torch.mean(loss)\n",
    "        batch_opt.zero_grad()\n",
    "        loss_list.append(float(loss_regu.data))\n",
    "        loss_regu.backward(retain_graph = True)\n",
    "        batch_opt.step()\n",
    "    ave_loss = np.average(np.array(loss_list))\n",
    "    ave_grad = np.sum(abs(batch_pert.grad).detach().cpu().numpy())\n",
    "    print('Gradient:',ave_grad,'Loss:', ave_loss)\n",
    "    if ave_grad == 0:\n",
    "        break\n",
    "\n",
    "noise = torch.clamp(batch_pert,-l_inf_r*2,l_inf_r*2)\n",
    "best_noise = noise.clone().detach().cpu()\n",
    "plt.imshow(np.transpose(noise[0].detach().cpu(),(1,2,0)))\n",
    "plt.show()\n",
    "print('Noise max val:',noise.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the trigger\n",
    "import time\n",
    "save_name = './checkpoint/best_noise'+'_'+ time.strftime(\"%m-%d-%H_%M_%S\",time.localtime(time.time())) \n",
    "np.save(save_name, best_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b8499",
   "metadata": {},
   "source": [
    "# Testing  attack effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9902b993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using this block if you only want to test the attack result.\n",
    "# import imageio\n",
    "# import cv2 as cv\n",
    "# best_noise = torch.zeros((1, 3, noise_size, noise_size), device=device)\n",
    "# noise_npy = np.load('./checkpoint/resnet18_trigger.npy')\n",
    "# best_noise = torch.from_numpy(noise_npy).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab66953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Poisoning amount use for the target class\n",
    "poison_amount = 25\n",
    "\n",
    "#Model uses for testing\n",
    "noise_testing_model = ResNet18().cuda()    \n",
    "\n",
    "#Training parameters\n",
    "training_epochs = 200\n",
    "training_lr = 0.1\n",
    "test_batch_size = 150\n",
    "\n",
    "#The multiple of noise amplification during testing\n",
    "multi_test = 3\n",
    "\n",
    "#random seed for testing stage\n",
    "random_seed = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e2fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "model = noise_testing_model\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=training_lr, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=training_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4075516",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_tensor = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "poi_ori_train = torchvision.datasets.CIFAR10(root=dataset_path, train=True, download=False, transform=transform_tensor)\n",
    "poi_ori_test = torchvision.datasets.CIFAR10(root=dataset_path, train=False, download=False, transform=transform_tensor)\n",
    "transform_after_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Poison traing\n",
    "random_poison_idx = random.sample(train_target_list, poison_amount)\n",
    "poison_train_target = poison_image(poi_ori_train,random_poison_idx,best_noise.cpu(),transform_after_train)\n",
    "print('Traing dataset size is:',len(poison_train_target),\" Poison numbers is:\",len(random_poison_idx))\n",
    "clean_train_loader = DataLoader(poison_train_target, batch_size=test_batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attack success rate testing\n",
    "test_non_target = list(np.where(np.array(test_label)!=lab)[0])\n",
    "test_non_target_change_image_label = poison_image_label(poi_ori_test,test_non_target,best_noise.cpu()*multi_test,lab,None)\n",
    "asr_loaders = torch.utils.data.DataLoader(test_non_target_change_image_label, batch_size=test_batch_size, shuffle=True, num_workers=2)\n",
    "print('Poison test dataset size is:',len(test_non_target_change_image_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean acc test dataset\n",
    "clean_test_loader = torch.utils.data.DataLoader(ori_test, batch_size=test_batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ab88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target clean test dataset\n",
    "test_target = list(np.where(np.array(test_label)==lab)[0])\n",
    "target_test_set = Subset(ori_test,test_target)\n",
    "target_test_loader = torch.utils.data.DataLoader(target_test_set, batch_size=test_batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import AverageMeter\n",
    "train_ACC = []\n",
    "test_ACC = []\n",
    "clean_ACC = []\n",
    "target_ACC = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d3cd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm.notebook.tqdm(range(training_epochs)):\n",
    "    # Train\n",
    "    model.train()\n",
    "    acc_meter = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "    pbar = tqdm.notebook.tqdm(clean_train_loader, total=len(clean_train_loader))\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        acc = (predicted == labels).sum().item()/labels.size(0)\n",
    "        acc_meter.update(acc)\n",
    "        loss_meter.update(loss.item())\n",
    "        pbar.set_description(\"Acc %.2f Loss: %.2f\" % (acc_meter.avg*100, loss_meter.avg))\n",
    "    train_ACC.append(acc_meter.avg)\n",
    "    print('Train_loss:',loss)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Testing attack effect\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for i, (images, labels) in enumerate(asr_loaders):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(images)\n",
    "            out_loss = criterion(logits,labels)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = correct / total\n",
    "    test_ACC.append(acc)\n",
    "    print('\\nAttack success rate %.2f' % (acc*100))\n",
    "    print('Test_loss:',out_loss)\n",
    "    \n",
    "    correct_clean, total_clean = 0, 0\n",
    "    for i, (images, labels) in enumerate(clean_test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(images)\n",
    "            out_loss = criterion(logits,labels)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total_clean += labels.size(0)\n",
    "            correct_clean += (predicted == labels).sum().item()\n",
    "    acc_clean = correct_clean / total_clean\n",
    "    clean_ACC.append(acc_clean)\n",
    "    print('\\nTest clean Accuracy %.2f' % (acc_clean*100))\n",
    "    print('Test_loss:',out_loss)\n",
    "    \n",
    "    correct_tar, total_tar = 0, 0\n",
    "    for i, (images, labels) in enumerate(target_test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(images)\n",
    "            out_loss = criterion(logits,labels)\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total_tar += labels.size(0)\n",
    "            correct_tar += (predicted == labels).sum().item()\n",
    "    acc_tar = correct_tar / total_tar\n",
    "    target_ACC.append(acc_tar)\n",
    "    print('\\nTarget test clean Accuracy %.2f' % (acc_tar*100))\n",
    "    print('Test_loss:',out_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa785df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ours -- higher_configureations\n",
    "from matplotlib import pyplot as plt\n",
    "half = np.arange(0,training_epochs)\n",
    "plt.figure(figsize=(12.5,8))\n",
    "plt.plot(half, np.asarray(train_ACC)[half], label='Training ACC', linestyle=\"-.\", marker=\"o\", linewidth=3.0, markersize = 8)\n",
    "plt.plot(half, np.asarray(test_ACC)[half], label='Attack success rate', linestyle=\"-.\", marker=\"o\", linewidth=3.0, markersize = 8)\n",
    "plt.plot(half, np.asarray(clean_ACC)[half], label='Clean test ACC', linestyle=\"-.\", marker=\"o\", linewidth=3.0, markersize = 8)\n",
    "plt.plot(half, np.asarray(target_ACC)[half], label='Target class clean test ACC', linestyle=\"-\", marker=\"o\", linewidth=3.0, markersize = 8)\n",
    "# plt.plot(half, np.asarray(test_unl_ACC)[half], label='protected test ACC', linestyle=\"-.\", marker=\"o\", linewidth=3.0, markersize = 8)\n",
    "plt.ylabel('ACC', fontsize=24)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.xlabel('Epoches', fontsize=24)\n",
    "plt.yticks(np.arange(0,1.1, 0.1),fontsize=20)\n",
    "plt.legend(fontsize=20,bbox_to_anchor=(1.016, 1.2),ncol=2)\n",
    "plt.grid(color=\"gray\", linestyle=\"-\")\n",
    "plt.show()\n",
    "\n",
    "dis_idx = clean_ACC.index(max(clean_ACC))\n",
    "print(train_ACC[dis_idx])\n",
    "print('attack',test_ACC[dis_idx])\n",
    "print(clean_ACC.index(max(clean_ACC)))\n",
    "print('all class clean', clean_ACC[dis_idx])\n",
    "print('target clean',target_ACC[dis_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
